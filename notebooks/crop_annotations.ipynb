{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare image and volume crops\n",
    "Here we will create simple tools for cropping input imagery and validate produces annotations.\n",
    "\n",
    "Every section has a safety mechanism in form of a flag which has to be changed manually to True for section to produce / change anything. \n",
    "\n",
    "## Extensions\n",
    "It is very recommended to have a few extensions to jupyter notebook enabled so that it is much easier to work with this toolset:\n",
    "* Table of Contents (2) - allows to easy orient oneself in many sections of this notebook\n",
    "* Initialization cells - makes sure that all setup cells are run so that you can go straight to cropping\n",
    "\n",
    "## Contents:\n",
    "1. Loading and setting up data paths\n",
    "2. Crop new data for annotation (dapi, membrane)\n",
    "3. Crop new random data for CLB\n",
    "4. Recrop using previous crops\n",
    "5. Establish voxel size\n",
    "6. Validate annotations\n",
    "7. Compare annotation data\n",
    "8. Prepare summary of all annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from notebook_utils import *\n",
    "\n",
    "import imageio\n",
    "import skimage.segmentation\n",
    "import skimage.filters\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup paths and common methods\n",
    "Here each new user can add its local paths to the dataset and code that are required by this toolset. The proper one is chosen as the one that actually exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import clb.utils\n",
    "from functools import lru_cache\n",
    "from clb.dataprep.readers import get_volume_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "data_root = pick_path([r'D:\\Fafa\\MIT\\CellDx', # fmroz\n",
    "                       r'C:\\MIT\\CLDX\\Data', # fmroz\n",
    "                       r'C:\\Annotations\\CLDX\\Crop_dataset', # patryk\n",
    "                       '/home/bartlomiej/clb_data/Crop_dataset' # bkuchnowski\n",
    "                      ])\n",
    "raw_data_root = pick_path([r'G:\\Dyski zespołu\\CLB\\Public', # fmroz\n",
    "                          r'G:\\Shared drives\\CLB\\Public', # fmroz\n",
    "                           '/home/bartlomiej/clb_data/raw_data' #bkuchnowski\n",
    "                          ])\n",
    "raw_annotation_root = pick_path([r\"G:\\Dyski zespołu\\CLB\\Public\\Datasets annotation\", # fmroz\n",
    "                                 r\"G:\\Shared drives\\CLB\\Public\\Datasets annotation\", # fmroz\n",
    "                                 r\"C:\\Annotations\\CLDX\\New_crops\", # patryk\n",
    "                                 r\"/home/bartlomiej/clb_data/New_crops\" # bkuchnowski\n",
    "                                ])\n",
    "annotation_root = pick_path([r\"G:\\Dyski zespołu\\CLB_Team\\Private\\_annotations\",  # fmroz\n",
    "                             r\"G:\\Shared drives\\CLB_Team\\Private\\_annotations\",  # fmroz\n",
    "                             r\"C:\\Annotations\\CLDX\\New_crops\", # patryk\n",
    "                             r\"/home/bartlomiej/clb_data/New_crops\" # bkuchnowski\n",
    "                            ])\n",
    "code_root = pick_path([r\"O:\\Code\\cldx-pilot\", # fmroz \n",
    "                       r\"C:\\MIT\\CLDX\\Code\\cldx-pilot\", # fmroz\n",
    "                       r\"C:\\Mit_Projects\\cldx-pilot\", # patryk\n",
    "                       r\"/home/bartlomiej/repos/cldx-pilot\" # bkuchnowski\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import clb.evaluate.evaluator_segment\n",
    "import clb.classify.utils\n",
    "def get_standard_datasets(annotation_root, series, in_gd = True):\n",
    "    if series is None:\n",
    "        series_list = os.listdir(annotation_root)\n",
    "        datasets_lists = [get_standard_datasets(annotation_root, s, in_gd) for s in series_list]\n",
    "        return sum(datasets_lists, [])\n",
    "    \n",
    "    annotation_base_dir = os.path.join(annotation_root, series)\n",
    "    input_dir = os.path.join(annotation_base_dir, \"Input\" if in_gd else \"images\")\n",
    "    labels_dir = os.path.join(annotation_base_dir, \"Annotations\" if in_gd else \"labels\")\n",
    "    return clb.evaluate.evaluator_segment.get_all_datasets(input_dir, labels_dir, input_dir, input_dir)\n",
    "\n",
    "def get_class_datasets(annotation_root, series, class_name):\n",
    "    annotation_base_dir = os.path.join(annotation_root, series)\n",
    "    input_dir = os.path.join(annotation_base_dir, \"input\")\n",
    "    labels_dir = os.path.join(annotation_base_dir, \"annotations\")\n",
    "    return clb.classify.utils.get_all_datasets(input_dir, labels_dir, input_dir, input_dir, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of cropping sets\n",
    "These sets represent one series of the data from which we crop data. It should correspond to the folder with data that we store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class CroppingSet:\n",
    "    def __init__(self, full_path_template, annotation_prefix, output_root_dir, upper_case=True):\n",
    "        self.full_path_template = full_path_template\n",
    "        self.annotation_prefix = annotation_prefix\n",
    "        self.output_root_dir = output_root_dir\n",
    "        self.input_name = \"Input\" if upper_case else \"input\"\n",
    "        self.annotations_name = \"Annotations\" if upper_case else \"annotations\"\n",
    "        self.raw_lif_path = None\n",
    "        self.raw_series = None\n",
    "        self.internal_name = os.path.basename(output_root_dir)\n",
    "    \n",
    "    def set_lif_file(self, lif_path, series):\n",
    "        \"\"\"\n",
    "        @param series: zero-based index of series in lif file\n",
    "        \"\"\"\n",
    "        self.raw_lif_path = lif_path\n",
    "        self.raw_series = series\n",
    "    \n",
    "    def get_multi_channels(self):\n",
    "        c1 = self.full_path_template.replace(\"CX\",\"C1\")\n",
    "        c2 = self.full_path_template.replace(\"CX\",\"C2\")\n",
    "        c3 = self.full_path_template.replace(\"CX\",\"C3\")\n",
    "        c4 = self.full_path_template.replace(\"CX\",\"C4\")\n",
    "        return (c1,c2,c3,c4)\n",
    "    \n",
    "    def get_raw_data(self):\n",
    "        if self.raw_lif_path is not None:\n",
    "            return get_volume_reader(self.raw_lif_path, series=self.raw_series)\n",
    "        return None\n",
    "    \n",
    "    def get_voxel_size(self):\n",
    "        if self.raw_lif_path is None:\n",
    "            return None\n",
    "        z,y,x = self.get_raw_data().voxel_size\n",
    "        return round(z, 2), round(y, 2), round(x, 2)\n",
    "    \n",
    "    def get_channel_data(self, channel_index, max_z=None):\n",
    "        if self.raw_lif_path is None:\n",
    "            path_to_full = self.get_multi_channels()[channel_index]\n",
    "            volume = imageio.volread(path_to_full)\n",
    "            if max_z is not None:\n",
    "                volume = volume[:max_z]\n",
    "            return volume\n",
    "        else:\n",
    "            if max_z is None:\n",
    "                volume_iter = self.get_raw_data()[:, channel_index]\n",
    "            else:\n",
    "                volume_iter = self.get_raw_data()[:max_z, channel_index]\n",
    "            return volume_iter.to_numpy()\n",
    "        \n",
    "    def get_datasets(self, include_non_annotated=False):\n",
    "        input_dir = os.path.join(self.output_root_dir, self.input_name)\n",
    "        labels_dir = os.path.join(self.output_root_dir, self.annotations_name)\n",
    "        if include_non_annotated:\n",
    "            labels_dir = input_dir\n",
    "        datasets = clb.evaluate.evaluator_segment.get_all_datasets(input_dir, labels_dir, input_dir, input_dir)\n",
    "        return datasets\n",
    "    \n",
    "    @property\n",
    "    def cropped_path_input(self):\n",
    "        return os.path.join(self.output_root_dir, self.input_name)\n",
    "    \n",
    "    @property\n",
    "    def cropped_path_annotation(self):\n",
    "        return os.path.join(self.output_root_dir, self.annotations_name)\n",
    "    \n",
    "    @lru_cache(5)\n",
    "    def get_data_shape(self):\n",
    "        return tuple(self.get_channel_data(0).shape)[:3]\n",
    "    \n",
    "    def set_voxel_size(self, infos):\n",
    "        dataset_voxel_size = self.get_voxel_size()\n",
    "        warning = []\n",
    "        if dataset_voxel_size is not None:\n",
    "            for info in infos:\n",
    "                info_voxel_size = getattr(info, 'voxel_size', None)\n",
    "                if info_voxel_size is not None and info_voxel_size != dataset_voxel_size:\n",
    "                    warning.append(\"Changed voxel size from {0} to {1}.\".format(info_voxel_size, dataset_voxel_size))\n",
    "                info.voxel_size = dataset_voxel_size\n",
    "        for warn in set(warning):\n",
    "            print (warn)\n",
    "    \n",
    "    def crop_by_infos(self, infos, channel_index=0):\n",
    "        # It also sets proper voxel size.\n",
    "        self.set_voxel_size(infos)\n",
    "        volume = self.get_channel_data(channel_index, max_z=infos[-1].z+1).copy()\n",
    "        return clb.cropping.VolumeROI.from_absolute_crop_with_padding(infos, volume).crop_volume\n",
    "    \n",
    "    def get_all_infos(self):\n",
    "        list_of_files = [clb.cropping.CropInfo.load(d.crop_info) for d in self.get_datasets(include_non_annotated=True)]\n",
    "        return [d for ds in list_of_files for d in ds]\n",
    "    \n",
    "    def get_overlap(self, infos):\n",
    "        return clb.cropping.CropInfo.overlap_volume_fraction(infos, self.get_all_infos())\n",
    "    \n",
    "    def crop_by_infos_merged(self, infos, channel_indices=[0]):\n",
    "        res = []\n",
    "        for index in channel_indices:\n",
    "            res.append(self.crop_by_infos(infos, index))\n",
    "        return np.squeeze(np.stack(res, -1))\n",
    "    \n",
    "    def crop_dataset(self, dataset, channel_index=0):\n",
    "        infos = clb.cropping.CropInfo.load(dataset.crop_info)\n",
    "        return self.crop_by_infos(infos, channel_index)\n",
    "\n",
    "    def duplicate_set(self, output_root_dir, annotation_prefix=None):\n",
    "        annotation_prefix = annotation_prefix or self.annotation_prefix\n",
    "        new_set = CroppingSet(self.full_path_template, annotation_prefix, output_root_dir, True)\n",
    "        new_set.input_name = self.input_name\n",
    "        new_set.annotations_name = self.annotations_name\n",
    "        \n",
    "        new_set.raw_lif_path = self.raw_lif_path\n",
    "        new_set.raw_series = self.raw_series\n",
    "        return new_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "raw_lif_file_T3_T5_T6 = os.path.join(raw_data_root, \"060718 To MIT.lif\")\n",
    "raw_lif_file_NewClass = os.path.join(raw_data_root, \"110718 more markers to test classification.lif\")\n",
    "raw_lif_file_512DenoisingData_S1_S2 = os.path.join(raw_data_root, \"Ab feasibility - Internal R-D_07022019.lif\")\n",
    "raw_lif_file_512DenoisingData_S3_S4_S5 = os.path.join(raw_data_root, \"Lectin feasiblity-expression_ETC cleared.lif\")\n",
    "raw_lif_file_Tonsil_CD = os.path.join(raw_data_root, \"Tonsil Ab test-1-23-19.lif\")\n",
    "\n",
    "path_to_full_T8_S1 = os.path.join(data_root, \"T8 S1 CX -#8T 2048 1024 0.5um more images.tif\")\n",
    "path_to_full_T8_S2 = os.path.join(data_root, \"T8 S2 CX -#8T 2048 1024 0.5um more images.tif\")\n",
    "path_to_full_T8_S3 = os.path.join(data_root, \"T8 S3 CX -#8T 2048 1024 0.5um more images.tif\")\n",
    "path_to_full_T3_S1 = os.path.join(data_root, \"T3 S1 CX -#3T 1024 step 0.5.tif\")\n",
    "path_to_full_T5_S1 = os.path.join(data_root, \"T5 S1 CX -#5T 1024 step 0.5.tif\")\n",
    "path_to_full_T6_S1 = os.path.join(data_root, \"T6 S1 CX -#6T 1024 step 0.5.tif\")\n",
    "path_to_full_NewClass_S1 = os.path.join(data_root, \"TestNewClasses S1 CX - Tonsil 175-2 1024.tif\")\n",
    "path_to_full_NewClass_S2 = os.path.join(data_root, \"TestNewClasses S2 CX - Tonsil 130 1024.tif\")\n",
    "path_to_full_NewClass_S3 = os.path.join(data_root, \"TestNewClasses S3 CX - FFPE 2048-1024.tif\")\n",
    "path_to_full_512DenoisingData_S1 = os.path.join(data_root, \"512DenoisingData S1 CX - Ab feasibility S8.tif\")\n",
    "path_to_full_512DenoisingData_S2 = os.path.join(data_root, \"512DenoisingData S2 CX - Ab feasibility S18.tif\")\n",
    "path_to_full_512DenoisingData_S3 = os.path.join(data_root, \"512DenoisingData S3 CX - Lectin feasiblity S2.tif\")\n",
    "path_to_full_512DenoisingData_S4 = os.path.join(data_root, \"512DenoisingData S4 CX - Lectin feasiblity S8.tif\")\n",
    "path_to_full_512DenoisingData_S5 = os.path.join(data_root, \"512DenoisingData S5 CX - Lectin feasiblity S12.tif\")\n",
    "\n",
    "annotation_prefix_T8_S1 = \"#8T S1 2048_1024 crop_\"\n",
    "annotation_prefix_T8_S2 = \"#8T S2 2048_1024 crop_\"\n",
    "annotation_prefix_T8_S3 = \"#8T S3 2048_1024 crop_\"\n",
    "annotation_prefix_T3_S1 = \"#3T S1 1024 crop_\"\n",
    "annotation_prefix_T5_S1 = \"#5T S1 1024 crop_\"\n",
    "annotation_prefix_T6_S1 = \"#6T S1 1024 crop_\"\n",
    "annotation_prefix_T3_S1_count = \"#3T S1 1024 crop_C_class_\"\n",
    "annotation_prefix_T5_S1_count = \"#5T S1 1024 crop_C_class_\"\n",
    "annotation_prefix_T6_S1_count = \"#6T S1 1024 crop_C_class_\"\n",
    "annotation_prefix_NewClass_S1 = \"TestNewClasses S1 1024 crop_\"\n",
    "annotation_prefix_NewClass_S2 = \"TestNewClasses S2 1024 crop_\"\n",
    "annotation_prefix_NewClass_S3 = \"TestNewClasses S3 2048_1024 crop_\"\n",
    "annotation_prefix_512DenoisingData_S1 = \"512DenoisingData S1 C1 crop_\"\n",
    "annotation_prefix_512DenoisingData_S2 = \"512DenoisingData S2 C1 crop_\"\n",
    "annotation_prefix_512DenoisingData_S3 = \"512DenoisingData S3 C1 crop_\"\n",
    "annotation_prefix_512DenoisingData_S4 = \"512DenoisingData S4 C1 crop_\"\n",
    "annotation_prefix_512DenoisingData_S5 = \"512DenoisingData S5 C1 crop_\"\n",
    "\n",
    "annotation_prefix_Tonsil_CD3_S2 = \"Tonsil_CD3 S2 crop_\"\n",
    "annotation_prefix_Tonsil_CD8_S3 = \"Tonsil_CD8 S3 crop_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "annotation_dir_T3_S1 = os.path.join(annotation_root, \"T3_S1\")\n",
    "annotation_dir_T5_S1 = os.path.join(annotation_root, \"T5_S1\")\n",
    "annotation_dir_T6_S1 = os.path.join(annotation_root, \"T6_S1\")\n",
    "annotation_dir_T8_S1 = os.path.join(annotation_root, \"T8_S1\")\n",
    "annotation_dir_T8_S2 = os.path.join(annotation_root, \"T8_S2\")\n",
    "annotation_dir_T8_S3 = os.path.join(annotation_root, \"T8_S3\")\n",
    "annotation_dir_NewClass_S1_Seg = os.path.join(annotation_root, \"TestNewClasses_S1\")\n",
    "annotation_dir_NewClass_S2_Seg = os.path.join(annotation_root, \"TestNewClasses_S2\")\n",
    "annotation_dir_512DenoisingData_S1 =  os.path.join(annotation_root, \"512DenoisingData_S1\")\n",
    "annotation_dir_512DenoisingData_S2 =  os.path.join(annotation_root, \"512DenoisingData_S2\")\n",
    "annotation_dir_512DenoisingData_S3 =  os.path.join(annotation_root, \"512DenoisingData_S3\")\n",
    "annotation_dir_512DenoisingData_S4 =  os.path.join(annotation_root, \"512DenoisingData_S4\")\n",
    "annotation_dir_512DenoisingData_S5 =  os.path.join(annotation_root, \"512DenoisingData_S5\")\n",
    "\n",
    "cropping_set_T3 = CroppingSet(path_to_full_T3_S1, annotation_prefix_T3_S1, annotation_dir_T3_S1)\n",
    "cropping_set_T3.set_lif_file(raw_lif_file_T3_T5_T6, 1)\n",
    "cropping_set_T5 = CroppingSet(path_to_full_T5_S1, annotation_prefix_T5_S1, annotation_dir_T5_S1)\n",
    "cropping_set_T5.set_lif_file(raw_lif_file_T3_T5_T6, 2)\n",
    "cropping_set_T6 = CroppingSet(path_to_full_T6_S1, annotation_prefix_T6_S1, annotation_dir_T6_S1)\n",
    "cropping_set_T6.set_lif_file(raw_lif_file_T3_T5_T6, 0)\n",
    "\n",
    "cropping_set_T8_1 = CroppingSet(path_to_full_T8_S1, annotation_prefix_T8_S1, annotation_dir_T8_S1)\n",
    "cropping_set_T8_2 = CroppingSet(path_to_full_T8_S2, annotation_prefix_T8_S2, annotation_dir_T8_S2)\n",
    "cropping_set_T8_3 = CroppingSet(path_to_full_T8_S3, annotation_prefix_T8_S3, annotation_dir_T8_S3)\n",
    "\n",
    "cropping_set_NewClass_S1_Seg = \\\n",
    "    CroppingSet(path_to_full_NewClass_S1, annotation_prefix_NewClass_S1, annotation_dir_NewClass_S1_Seg, upper_case=False)\n",
    "cropping_set_NewClass_S1_Seg.set_lif_file(raw_lif_file_NewClass, 0)\n",
    "cropping_set_NewClass_S2_Seg = \\\n",
    "    CroppingSet(path_to_full_NewClass_S2, annotation_prefix_NewClass_S2, annotation_dir_NewClass_S2_Seg, upper_case=False)\n",
    "cropping_set_NewClass_S2_Seg.set_lif_file(raw_lif_file_NewClass, 1)\n",
    "\n",
    "cropping_set_512DenoisingData_S1 = CroppingSet(path_to_full_512DenoisingData_S1, annotation_prefix_512DenoisingData_S1, annotation_dir_512DenoisingData_S1, upper_case=False)\n",
    "cropping_set_512DenoisingData_S1.set_lif_file(raw_lif_file_512DenoisingData_S1_S2, 7)\n",
    "cropping_set_512DenoisingData_S2 = CroppingSet(path_to_full_512DenoisingData_S2, annotation_prefix_512DenoisingData_S2, annotation_dir_512DenoisingData_S2, upper_case=False)\n",
    "cropping_set_512DenoisingData_S2.set_lif_file(raw_lif_file_512DenoisingData_S1_S2, 17)\n",
    "cropping_set_512DenoisingData_S3 = CroppingSet(path_to_full_512DenoisingData_S3, annotation_prefix_512DenoisingData_S3, annotation_dir_512DenoisingData_S3, upper_case=False)\n",
    "cropping_set_512DenoisingData_S3.set_lif_file(raw_lif_file_512DenoisingData_S3_S4_S5, 1)\n",
    "cropping_set_512DenoisingData_S4 = CroppingSet(path_to_full_512DenoisingData_S4, annotation_prefix_512DenoisingData_S4, annotation_dir_512DenoisingData_S4, upper_case=False)\n",
    "cropping_set_512DenoisingData_S4.set_lif_file(raw_lif_file_512DenoisingData_S3_S4_S5, 7)\n",
    "cropping_set_512DenoisingData_S5 = CroppingSet(path_to_full_512DenoisingData_S5, annotation_prefix_512DenoisingData_S5, annotation_dir_512DenoisingData_S5, upper_case=False)\n",
    "cropping_set_512DenoisingData_S5.set_lif_file(raw_lif_file_512DenoisingData_S3_S4_S5, 11)\n",
    "\n",
    "all_segmentation_sets = [cropping_set_T8_1, cropping_set_T8_2, cropping_set_T8_3,\n",
    "                     cropping_set_NewClass_S1_Seg, cropping_set_NewClass_S2_Seg,\n",
    "                     cropping_set_T3, cropping_set_T5, cropping_set_T6,\n",
    "                     cropping_set_512DenoisingData_S1, cropping_set_512DenoisingData_S2, cropping_set_512DenoisingData_S3, cropping_set_512DenoisingData_S4, cropping_set_512DenoisingData_S5\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "annotation_dir_T3_S1_extend = os.path.join(annotation_root, \"T3_S1_extended\")\n",
    "annotation_dir_T5_S1_extend = os.path.join(annotation_root, \"T5_S1_extended\")\n",
    "annotation_dir_T6_S1_extend = os.path.join(annotation_root, \"T6_S1_extended\")\n",
    "annotation_dir_T3_S1_count = os.path.join(annotation_root, \"T3_S1_counters\")\n",
    "annotation_dir_T5_S1_count = os.path.join(annotation_root, \"T5_S1_counters\")\n",
    "annotation_dir_T6_S1_count = os.path.join(annotation_root, \"T6_S1_counters\")\n",
    "annotation_dir_NewClass_S1_Class_Test = os.path.join(annotation_root, \"TestNewClasses_S1_test\")\n",
    "annotation_dir_NewClass_S2_Class_Test = os.path.join(annotation_root, \"TestNewClasses_S2_test\")\n",
    "annotation_dir_NewClass_S1 = os.path.join(annotation_root, \"TestNewClasses_S1_extended\")\n",
    "annotation_dir_NewClass_S2 = os.path.join(annotation_root, \"TestNewClasses_S2_extended\")\n",
    "annotation_dir_NewClass_S3 = os.path.join(annotation_root, \"TestNewClasses_S3_extended\")\n",
    "\n",
    "annotation_dir_Tonsil_CD3_S2 = os.path.join(annotation_root, \"Tonsil_CD3_S2_extended\")\n",
    "annotation_dir_Tonsil_CD8_S3 = os.path.join(annotation_root, \"Tonsil_CD8_S3_extended\")\n",
    "\n",
    "cropping_set_T3_count = cropping_set_T3.duplicate_set(annotation_dir_T3_S1_count, annotation_prefix_T3_S1_count)\n",
    "cropping_set_T5_count = cropping_set_T5.duplicate_set(annotation_dir_T5_S1_count, annotation_prefix_T5_S1_count)\n",
    "cropping_set_T6_count = cropping_set_T6.duplicate_set(annotation_dir_T6_S1_count, annotation_prefix_T6_S1_count)\n",
    "cropping_set_T3_extend = cropping_set_T3.duplicate_set(annotation_dir_T3_S1_extend)\n",
    "cropping_set_T5_extend = cropping_set_T5.duplicate_set(annotation_dir_T5_S1_extend)\n",
    "cropping_set_T6_extend = cropping_set_T6.duplicate_set(annotation_dir_T6_S1_extend)\n",
    "\n",
    "cropping_set_NewClass_S1 = cropping_set_NewClass_S1_Seg.duplicate_set(annotation_dir_NewClass_S1)\n",
    "cropping_set_NewClass_S2 = cropping_set_NewClass_S1_Seg.duplicate_set(annotation_dir_NewClass_S2)\n",
    "cropping_set_NewClass_S3 = \\\n",
    "    CroppingSet(path_to_full_NewClass_S3, annotation_prefix_NewClass_S3, annotation_dir_NewClass_S3, upper_case=False)\n",
    "cropping_set_NewClass_S3.set_lif_file(raw_lif_file_NewClass, 2)\n",
    "\n",
    "cropping_set_NewClass_S1_Class_Test = cropping_set_NewClass_S1.duplicate_set(annotation_dir_NewClass_S1_Class_Test)\n",
    "cropping_set_NewClass_S2_Class_Test = cropping_set_NewClass_S2.duplicate_set(annotation_dir_NewClass_S2_Class_Test)\n",
    "\n",
    "cropping_set_Tonsil_CD3_S2 = CroppingSet(None, annotation_prefix_Tonsil_CD3_S2, annotation_dir_Tonsil_CD3_S2, upper_case=False)\n",
    "cropping_set_Tonsil_CD3_S2.set_lif_file(raw_lif_file_Tonsil_CD, 1)\n",
    "cropping_set_Tonsil_CD8_S3 = CroppingSet(None, annotation_prefix_Tonsil_CD8_S3, annotation_dir_Tonsil_CD8_S3, upper_case=False)\n",
    "cropping_set_Tonsil_CD8_S3.set_lif_file(raw_lif_file_Tonsil_CD, 2)\n",
    "\n",
    "\n",
    "all_classification_sets = [\n",
    "                     cropping_set_NewClass_S1, cropping_set_NewClass_S2, cropping_set_NewClass_S3,\n",
    "                     cropping_set_NewClass_S1_Class_Test, cropping_set_NewClass_S2_Class_Test,\n",
    "                     cropping_set_T3_count, cropping_set_T5_count, cropping_set_T6_count,\n",
    "                     cropping_set_T3_extend, cropping_set_T5_extend, cropping_set_T6_extend,\n",
    "                     cropping_set_Tonsil_CD3_S2, cropping_set_Tonsil_CD8_S3\n",
    "                    ]\n",
    "\n",
    "all_cropping_sets = all_segmentation_sets + all_classification_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop new data for annotation (dapi, membrane)\n",
    "This allows to crop at the specific position and save dapi and membrane. It is actually the first implementation and should not be used much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clb.cropping\n",
    "importlib.reload(clb.cropping)\n",
    "crop_new_data = False  # IT IS DEPRECATED USE ONLY IF REALLY NEED "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set from which dataset and coordinates of the crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_full_dapi = cropping_set_T6.get_multi_channels()[0]\n",
    "path_to_full_memb = cropping_set_T6.get_multi_channels()[2]\n",
    "\n",
    "annotation_dir = cropping_set_T6.cropped_path_input\n",
    "annotation_prefix = cropping_set_T6.annotation_prefix\n",
    "\n",
    "crop_number = \"52\"\n",
    "eval_only_crop_number = \"E5\"\n",
    "annotation_name = annotation_prefix + crop_number + \" 0.5um_dapi.yaml\"\n",
    "annotation_path = os.path.join(annotation_dir, annotation_name)\n",
    "\n",
    "if crop_new_data:\n",
    "    x = 25\n",
    "    y = 693\n",
    "    start_z = 40\n",
    "    thickness = 5\n",
    "    crops_volume = clb.cropping.CropInfo.create_volume(y,x,200,200,range(start_z, start_z+thickness, 1))\n",
    "    print('crops_len =', len(crops_volume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crop_new_data:\n",
    "    first_image = imageio.volread(path_to_full_dapi)[crops_volume[0].z].copy()\n",
    "    first_crop = crops_volume[0].crop(first_image, from_volume=False)\n",
    "    print(first_crop.shape)\n",
    "#show_all(1,1, first_crop, scale=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save crop and crop info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crop_new_data:\n",
    "    print(annotation_name)\n",
    "    output_path_dapi = annotation_path[:-10] + \"_dapi.tif\"\n",
    "    output_path_mem = annotation_path[:-10] + \"_mem.tif\"\n",
    "    dapis = []\n",
    "    membranes = []\n",
    "    \n",
    "    zs = [crop.z for crop in crops_volume]\n",
    "    min_z, max_z = zs[0], zs[-1]+1\n",
    "    \n",
    "    loaded_dapi = imageio.volread(path_to_full_dapi)[:max_z].copy()\n",
    "    loaded_memb = imageio.volread(path_to_full_memb)[:max_z].copy()\n",
    "    \n",
    "    for crop in tqdm(crops_volume):\n",
    "        cropped_dapi = crop.crop(loaded_dapi)\n",
    "        cropped_membrane = crop.crop(loaded_memb)\n",
    "        dapis.append(cropped_dapi)\n",
    "        membranes.append(cropped_membrane)\n",
    "    print(\"saving\", output_path_dapi)\n",
    "    imageio.mimwrite(output_path_dapi, dapis)\n",
    "    imageio.mimwrite(output_path_mem, membranes)\n",
    "    print(\"saving\", annotation_path)\n",
    "    clb.cropping.CropInfo.save(crops_volume, annotation_path)\n",
    "    # save info also\n",
    "    \n",
    "    show_all(1,2, dapis[0], membranes[0], scale=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crop new random data for CLB\n",
    "This section allows to crop randomly from entire volume - the random proposal is presented and it can be approved and saved.\n",
    "The number of the crops should then be manually incremented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import clb.cropping\n",
    "importlib.reload(clb.cropping)\n",
    "\n",
    "def show_crop(cropping_set, size_z, crops_volume, channels):\n",
    "    first_crop = cropping_set.crop_by_infos_merged(crops_volume, channels)\n",
    "    print('data_shape = ', cropping_set.get_data_shape())\n",
    "    print('crop_shape =', first_crop.shape)\n",
    "    print('crop_info =', crops_volume[0])\n",
    "    print('overlap_with_previous = {0} %'.format(100 * cropping_set.get_overlap(crops_volume)))\n",
    "    return crops_volume, first_crop[size_z // 2]\n",
    "\n",
    "def get_and_show_random_crop(cropping_set, size_z, size_y, size_x, channels):\n",
    "    crops_volume = clb.cropping.CropInfo.create_random_volume(cropping_set.get_data_shape(), size_y,size_x,size_z)\n",
    "    return show_crop(cropping_set, size_z, crops_volume, channels)\n",
    "\n",
    "def get_and_show_crop(cropping_set, left_top, crop_shape, channels):\n",
    "    slices = range(left_top[0], left_top[0]+crop_shape[0], 1)\n",
    "    crops_volume = clb.cropping.CropInfo.create_volume(left_top[1],left_top[2],crop_shape[1],crop_shape[2],slices)\n",
    "    return show_crop(cropping_set, size_z, crops_volume, channels)\n",
    "\n",
    "def save_croped_data(cropping_set, crops_volume, channels, number_desc, output_input, \n",
    "                     output_view=None, view_channels=None, view_deeper=0):\n",
    "    os.makedirs(output_input, exist_ok=True)\n",
    "    if output_view is not None:\n",
    "        os.makedirs(output_view, exist_ok=True)\n",
    "    \n",
    "    output_name = cropping_set.annotation_prefix + number_desc\n",
    "    annotation_name = output_name + \".yaml\"\n",
    "    annotation_path = os.path.join(output_input, annotation_name)\n",
    "    output_path_image_name = output_name + \".tif\"\n",
    "    output_path_image_path = os.path.join(output_input, output_path_image_name)\n",
    "    if output_view is not None and view_channels is not None:\n",
    "        output_path_image_full_name = output_name + \"_view.tif\"\n",
    "        output_path_image_full_path = os.path.join(output_view, output_path_image_full_name)\n",
    "    print(annotation_name)\n",
    "\n",
    "    if os.path.isfile(output_path_image_path):\n",
    "        raise Exception(\"Requested save path exists!\\n\" + \n",
    "                        output_path_image_path + \"\\n\\nBe carefull as annotations may already exist.\")\n",
    "    \n",
    "    if os.path.isfile(annotation_path):\n",
    "        raise Exception(\"Requested save path exists!\\n\" + \n",
    "                        annotation_path + \"\\n\\nBe carefull as annotations may already exist.\")\n",
    "    \n",
    "    crop_all_channels = cropping_set.crop_by_infos_merged(crops_volume, channels)\n",
    "    print(\"saving\", output_path_image_path)\n",
    "    imageio.mimwrite(output_path_image_path, crop_all_channels)\n",
    "    print(\"saving\", annotation_path)\n",
    "    clb.cropping.CropInfo.save(crops_volume, annotation_path)\n",
    "    \n",
    "    if output_view is not None and view_channels is not None:\n",
    "        print(\"saving view\", output_path_image_full_path)\n",
    "        if view_deeper > 0:\n",
    "            crops_volume = clb.cropping.CropInfo.extend_infos(crops_volume, (view_deeper, 0, 0))\n",
    "        crop_view_channels = cropping_set.crop_by_infos_merged(crops_volume, view_channels)\n",
    "        crop_view_channels = clb.dataprep.utils.ensure_3d_rgb(crop_view_channels)\n",
    "\n",
    "        imageio.mimwrite(output_path_image_full_path, crop_view_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop new data for segmentation 1um (dapi) with view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clb.cropping\n",
    "importlib.reload(clb.cropping)\n",
    "crop_new_segment_data = False\n",
    "crop_new_segment_512_prefix = \"seg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_set = cropping_set_512DenoisingData_S5\n",
    "number_prefix = crop_new_segment_512_prefix\n",
    "crop_number_for_random_crops = 76\n",
    "\n",
    "crop_number_desc = number_prefix + '_' + str(crop_number_for_random_crops)\n",
    "\n",
    "size_z, size_y, size_x = (5, 128, 128) \n",
    "sizes = size_z, size_y, size_x\n",
    "channels = [0]  # only dapi\n",
    "view_channels = [0,1]  # all to help, None if not needed\n",
    "view_deeper = 3\n",
    "random_output_root = cropping_set.output_root_dir\n",
    "random_output_input = os.path.join(random_output_root, \"input\")\n",
    "random_output_view = os.path.join(random_output_root, \"view\")\n",
    "\n",
    "fig = None\n",
    "if crop_new_segment_data:\n",
    "    print (crop_number_desc)\n",
    "    #crops_volume, first_crop = get_and_show_crop(cropping_set, (101, 220, 20), sizes, channels)\n",
    "    crops_volume, first_crop = get_and_show_random_crop(cropping_set, size_z, size_y, size_x, channels)\n",
    "    fig = show_all(1,1, first_crop, scale=7)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crop_new_segment_data:\n",
    "    os.makedirs(random_output_input, exist_ok=True)\n",
    "    os.makedirs(os.path.join(random_output_root, \"annotations\"), exist_ok=True)\n",
    "    save_croped_data(cropping_set, crops_volume, channels, crop_number_desc, \n",
    "                     random_output_input, random_output_view, view_channels, view_deeper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop new data for CLB counting validation (dapi, PAN, KI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clb.cropping\n",
    "importlib.reload(clb.cropping)\n",
    "crop_new_counting_data = False\n",
    "instance_prefix = 'instance'\n",
    "class_prefix = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_set = cropping_set_T6\n",
    "number_prefix = instance_prefix\n",
    "counter_crop_number = 13\n",
    "\n",
    "counter_crop_number_desc = 'C_' + number_prefix + '_' + str(counter_crop_number)\n",
    "if number_prefix == class_prefix:\n",
    "    size_z, size_y, size_x = (30, 400, 400) # class size\n",
    "else:\n",
    "    size_z, size_y, size_x = (20, 200, 200) # instance size\n",
    "channels = [0,1,2]\n",
    "\n",
    "counter_output_root = cropping_set.output_root_dir + \"_counters\"\n",
    "counter_output_input = os.path.join(counter_output_root, \"input\")\n",
    "\n",
    "fig = None\n",
    "if crop_new_counting_data:\n",
    "    print (counter_crop_number_desc)\n",
    "    crops_volume, first_crop = get_and_show_random_crop(cropping_set, size_z, size_y, size_x, channels)\n",
    "    fig = show_all(1,1, first_crop, scale=7)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save crop and crop info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crop_new_counting_data:\n",
    "    os.makedirs(counter_output_input, exist_ok=True)\n",
    "    os.makedirs(os.path.join(counter_output_root, \"counters\"), exist_ok=True)\n",
    "    \n",
    "    save_croped_data(cropping_set, crops_volume, channels, counter_crop_number_desc, counter_output_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop new data for CLB classification (dapi, PAN, KI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clb.cropping\n",
    "importlib.reload(clb.cropping)\n",
    "crop_new_random_data = False\n",
    "crop_rand_prefix = 'class_rand'\n",
    "root_dir_suffix = \"_extended\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_set = cropping_set_T8_2\n",
    "number_prefix = crop_rand_prefix\n",
    "random_crop_number = 5\n",
    "\n",
    "crop_number_desc = number_prefix + '_' + str(random_crop_number)\n",
    "\n",
    "size_z, size_y, size_x = (15, 400, 400) # class size\n",
    "channels = [0,1,2]\n",
    "random_output_root = cropping_set.output_root_dir + root_dir_suffix\n",
    "random_output_input = os.path.join(random_output_root, \"input\")\n",
    "\n",
    "fig = None\n",
    "if crop_new_random_data:\n",
    "    print (crop_number_desc)\n",
    "    crops_volume, first_crop = get_and_show_random_crop(cropping_set, size_z, size_y, size_x, channels)\n",
    "    fig = show_all(1,1, first_crop, scale=7)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crop_new_random_data:\n",
    "    os.makedirs(random_output_input, exist_ok=True)\n",
    "    os.makedirs(os.path.join(random_output_root, \"annotations\"), exist_ok=True)\n",
    "    \n",
    "    save_croped_data(cropping_set, crops_volume, channels, crop_number_desc, random_output_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop new data for CLB classification (dapi, PDL1-AF568, CD8-AF647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clb.cropping\n",
    "importlib.reload(clb.cropping)\n",
    "crop_new_pdl_cd_data = False\n",
    "crop_pdlcd_prefix = 'class_pdlcd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_set = cropping_set_Tonsil_CD8_S3\n",
    "number_prefix = crop_pdlcd_prefix\n",
    "random_crop_number = 30\n",
    "\n",
    "crop_number_desc = number_prefix + '_' + str(random_crop_number)\n",
    "\n",
    "size_z, size_y, size_x = (15, 600, 600) # class size\n",
    "sizes = size_z, size_y, size_x\n",
    "channels = [0,1,3]\n",
    "random_output_root = cropping_set.output_root_dir\n",
    "random_output_input = os.path.join(random_output_root, \"input\")\n",
    "\n",
    "fig = None\n",
    "if crop_new_pdl_cd_data:\n",
    "    print (crop_number_desc)\n",
    "    #crops_volume, first_crop = get_and_show_crop(cropping_set, (101, 220, 20), sizes, channels)\n",
    "    crops_volume, first_crop = get_and_show_random_crop(cropping_set, size_z, size_y, size_x, channels)\n",
    "    fig = show_all(1,1, first_crop, scale=7)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if crop_new_pdl_cd_data:\n",
    "    os.makedirs(random_output_input, exist_ok=True)\n",
    "    os.makedirs(os.path.join(random_output_root, \"annotations\"), exist_ok=True)\n",
    "    \n",
    "    save_croped_data(cropping_set, crops_volume, channels, crop_number_desc, random_output_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recrop using previous crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(clb.cropping)\n",
    "\n",
    "def extend_crop_info(infos, padding_3d):\n",
    "    return CropInfo.extend_infos(infos, padding_3d)\n",
    "    \n",
    "def get_extended_crop_info(dataset, padding_3d):\n",
    "    infos = clb.cropping.CropInfo.load(dataset.crop_info)\n",
    "    return extend_crop_info(infos, padding_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recrop instance crops for classification crops (dapi, PAN, KI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recrop_new_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_info_index_by_z(infos, z):\n",
    "    return [i.z for i in infos].index(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extended_labels(dataset, padding_3d, original_data_shape):\n",
    "    infos = clb.cropping.CropInfo.load(dataset.crop_info)\n",
    "    extended_infos = get_extended_crop_info(dataset, padding_3d)\n",
    "    labels = imageio.volread(dataset.gt)\n",
    "    \n",
    "    # keep them inside bounds\n",
    "    extended_infos = clb.cropping.CropInfo.restrict_infos(extended_infos, original_data_shape)\n",
    "    \n",
    "    res = clb.cropping.CropInfo.empty_volume(extended_infos)\n",
    "    pad_z, pad_y, pad_x = padding_3d\n",
    "    l_index = 0\n",
    "    \n",
    "    for org_info in infos:\n",
    "        z = org_info.z\n",
    "        new_z_index = find_info_index_by_z(extended_infos, z)\n",
    "        same_slice_ext_info = extended_infos[new_z_index]\n",
    "        \n",
    "        y_min = org_info.y - same_slice_ext_info.y\n",
    "        y_max = y_min + org_info.shape[0]\n",
    "        \n",
    "        x_min = org_info.x - same_slice_ext_info.x\n",
    "        x_max = x_min + org_info.shape[1]\n",
    "        \n",
    "        res[new_z_index][y_min:y_max, x_min:x_max] = labels[l_index]\n",
    "        l_index += 1\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_all_channels_labels(cropping_set, dataset, padding_3d, data_shape):\n",
    "    extended_crop_infos = get_extended_crop_info(dataset, padding_3d)\n",
    "    channel_dapi = cropping_set.crop_by_infos(extended_crop_infos, 0)\n",
    "    channel_kpi = cropping_set.crop_by_infos(extended_crop_infos, 1)\n",
    "    channel_pan = cropping_set.crop_by_infos(extended_crop_infos, 2)\n",
    "    \n",
    "    labels = get_extended_labels(dataset, padding_3d, data_shape)\n",
    "    return channel_dapi, channel_kpi, channel_pan, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropping_set = cropping_set_T3\n",
    "datasets = cropping_set.get_datasets()[5:6]\n",
    "padding = (8, 100, 100)\n",
    "\n",
    "extended_output_root = cropping_set.output_root_dir + \"_extended\"\n",
    "extended_output_input = os.path.join(extended_output_root,\"Input\")\n",
    "os.makedirs(extended_output_input, exist_ok=True)\n",
    "os.makedirs(os.path.join(extended_output_root,\"Annotations\"), exist_ok=True)\n",
    "\n",
    "if recrop_new_data:\n",
    "    print (\"Extending by\", padding)\n",
    "    for dataset in datasets:\n",
    "        print (\"Recropping extended for...\", dataset.input, end=' ')\n",
    "        extended_crop_infos = get_extended_crop_info(dataset, padding)\n",
    "        dapi, kpi, pan, labels = crop_all_channels_labels(cropping_set, dataset, padding, cropping_set.get_data_shape())\n",
    "        print (\" done.\")\n",
    "        \n",
    "        # reduce lut input to one channel\n",
    "        print (dapi.shape, dapi.dtype)\n",
    "        dapi_single = dapi #np.amax(dapi, 3)\n",
    "        kpi_single = kpi #np.amax(kpi, 3)\n",
    "        pan_single = pan #np.amax(pan, 3)\n",
    "        \n",
    "        old_annotation_name = os.path.basename(dataset.input)\n",
    "        \n",
    "        new_annotation_name = old_annotation_name[:-4] + \"_stack.tif\"\n",
    "        output_path_stack = os.path.join(extended_output_input, new_annotation_name)\n",
    "        #output_stack = np.stack([dapi_single, kpi_single, pan_single, labels], axis=3)\n",
    "        output_stack = np.stack([dapi_single, kpi_single, pan_single], axis=3)\n",
    "        \n",
    "        print(\"saving\", output_path_stack)\n",
    "        imageio.mimwrite(output_path_stack, output_stack)\n",
    "        \n",
    "        new_labels_name = old_annotation_name[:-4] + \"_labels.tif\"\n",
    "        output_path_labels = os.path.join(extended_output_input, new_labels_name)\n",
    "        print(\"saving\", output_path_labels)\n",
    "        imageio.mimwrite(output_path_labels, labels)\n",
    "        \n",
    "        new_info_name = old_annotation_name[:-4] + \"_stack.yaml\"\n",
    "        output_path_yaml = os.path.join(extended_output_input, new_info_name)\n",
    "        print(\"saving\", output_path_yaml)\n",
    "        clb.cropping.CropInfo.save(extended_crop_infos, output_path_yaml)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recrop classification crops for instance crops (dapi, PDL1, CD8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_splitted_crop_info(dataset, desired_shape, tile_num):\n",
    "    infos = clb.cropping.CropInfo.load(dataset.crop_info)\n",
    "    tile_count_x = infos[0].shape[-1] // desired_shape[-1]\n",
    "    \n",
    "    tile_num_x = tile_num % tile_count_x\n",
    "    tile_num_y = tile_num // tile_count_x\n",
    "    \n",
    "    y = infos[0].y\n",
    "    x = infos[0].x\n",
    "    \n",
    "    dz_size, dheight, dwidth = desired_shape\n",
    "    y += tile_num_y * dheight\n",
    "    x += tile_num_x * dwidth\n",
    "    \n",
    "    zs = [i.z for i in infos]\n",
    "    zs = zs[(len(zs) - dz_size) // 2:]\n",
    "    zs = zs[:dz_size]\n",
    "    \n",
    "    return clb.cropping.CropInfo.create_volume(y, x, dheight, dwidth, zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(clb.cropping)\n",
    "recrop_dapi_data = False\n",
    "\n",
    "cropping_set_classes = cropping_set_NewClass_S2\n",
    "cropping_set_seg = cropping_set_NewClass_S2_Seg\n",
    "datasets = cropping_set_classes.get_datasets()[0:3]\n",
    "\n",
    "shape = (3, 200, 200)\n",
    "number_of_tiles = 9\n",
    "channels = [0, 1, 3]\n",
    "view_extend = (4, 0, 0)\n",
    "\n",
    "instance_output_root = cropping_set_seg.output_root_dir\n",
    "instance_output_input = os.path.join(instance_output_root,\"input\")\n",
    "print(instance_output_input)\n",
    "os.makedirs(instance_output_input, exist_ok=True)\n",
    "os.makedirs(os.path.join(instance_output_root,\"annotations\"), exist_ok=True)\n",
    "\n",
    "if recrop_dapi_data:\n",
    "    print (\"Splitting into\", shape)\n",
    "    for dataset in datasets:\n",
    "        print (\"Splitting for... \", os.path.basename(dataset.input))\n",
    "        for cube_num in range(number_of_tiles):\n",
    "            old_annotation_name = os.path.basename(dataset.input)\n",
    "            new_annotation_name = old_annotation_name[:-4] + \"_tile_{0}.tif\".format(cube_num)\n",
    "            \n",
    "            new_info_name = old_annotation_name[:-4] + \"_tile_{0}.yaml\".format(cube_num)\n",
    "            output_path_yaml = os.path.join(instance_output_input, new_info_name)\n",
    "            \n",
    "            output_path_dapi = os.path.join(instance_output_input, new_annotation_name)\n",
    "            \n",
    "            if os.path.isfile(output_path_dapi) or os.path.isfile(output_path_yaml):\n",
    "                print(\"Requested save path exists!\", output_path_dapi)\n",
    "                continue\n",
    "              \n",
    "            print(\"saving\", output_path_dapi)\n",
    "            one_cube_infos = get_splitted_crop_info(dataset, shape, cube_num)\n",
    "            cube_dapi = cropping_set_seg.crop_by_infos(one_cube_infos, 0)\n",
    "            imageio.mimwrite(output_path_dapi, cube_dapi)\n",
    "\n",
    "            print(\"saving\", output_path_yaml)\n",
    "            clb.cropping.CropInfo.save(one_cube_infos, output_path_yaml)\n",
    "            \n",
    "            # get extended with all channels\n",
    "            new_deeper_name = old_annotation_name[:-4] + \"_tile_{0}_view.tif\".format(cube_num)\n",
    "            output_path_deeper = os.path.join(instance_output_input, new_deeper_name)\n",
    "            \n",
    "            print(\"saving\", output_path_deeper)\n",
    "            deeper_cube_infos = extend_crop_info(one_cube_infos, view_extend)\n",
    "            cube_channels_data = []\n",
    "            for channel in channels:\n",
    "                cube_channels_data.append(cropping_set_seg.crop_by_infos(deeper_cube_infos, channel))\n",
    "            output_stack = np.stack(cube_channels_data, axis=3)\n",
    "\n",
    "            imageio.mimwrite(output_path_deeper, output_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish voxel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_crop_voxel_size(crop_info_path):\n",
    "    crop_infos = clb.cropping.CropInfo.load(crop_info_path)\n",
    "    voxel_sizes = list(set([getattr(info, 'voxel_size', None) for info in crop_infos]))\n",
    "    if len(voxel_sizes) != 1:\n",
    "        raise Exception(\"Different voxel sizes in slices of the same 3d crop.\")\n",
    "    return voxel_sizes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect crop voxel size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inspect_crop_voxel_sizes = True\n",
    "\n",
    "def show_crops_voxel_size(cropping_set):\n",
    "    datasets_infos = [d.crop_info for d in cropping_set.get_datasets()]\n",
    "    print(\"Dataset:\", cropping_set.internal_name, \"voxel size:\", cropping_set.get_voxel_size())\n",
    "    for info in datasets_infos:\n",
    "        info_voxel_size = get_crop_voxel_size(info)\n",
    "        print('\\t',os.path.basename(info), \"has voxel size:\", info_voxel_size)\n",
    "\n",
    "if inspect_crop_voxel_sizes:   \n",
    "    for cropping_set in all_cropping_sets:\n",
    "        show_crops_voxel_size(cropping_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set voxel sizes for all crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "set_voxel_sizes_all_crops = False\n",
    "\n",
    "def set_crops_voxel_size(cropping_set, override_voxel_size=None):\n",
    "    dataset_voxel_size = cropping_set.get_voxel_size()\n",
    "    print(\"Dataset:\", cropping_set.internal_name, \"voxel size:\", dataset_voxel_size)\n",
    "    \n",
    "    voxel_size_to_set = override_voxel_size or dataset_voxel_size\n",
    "    \n",
    "    datasets_infos = [d.crop_info for d in cropping_set.get_datasets()]\n",
    "    for info in datasets_infos:\n",
    "        info_voxel_size = get_crop_voxel_size(info)\n",
    "        print('\\t',os.path.basename(info), \"has voxel size:\", info_voxel_size)\n",
    "        if info_voxel_size != voxel_size_to_set:\n",
    "            crop_infos = clb.cropping.CropInfo.load(info)\n",
    "            for i in crop_infos:\n",
    "                i.voxel_size = voxel_size_to_set\n",
    "            clb.cropping.CropInfo.save(crop_infos, info)\n",
    "            print('\\t', \"Changed to:\", get_crop_voxel_size(info))\n",
    "            \n",
    "if set_voxel_sizes_all_crops:\n",
    "    for cropping_set in [cs for cs in all_cropping_sets if cs.raw_lif_path is not None]:\n",
    "        set_crops_voxel_size(cropping_set)\n",
    "        \n",
    "#set_crops_voxel_size(cropping_set_T8_1, override_voxel_size=(0.5, 0.46, 0.46))\n",
    "#set_crops_voxel_size(cropping_set_T8_2, override_voxel_size=(0.5, 0.46, 0.46))\n",
    "#set_crops_voxel_size(cropping_set_T8_3, override_voxel_size=(0.5, 0.46, 0.46))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def validate_crop_info_in_bounds(d, y_size, x_size):\n",
    "    crop_info = clb.cropping.CropInfo.load(d.input[:-4] + '.yaml')[0]\n",
    "    restricted = crop_info.restrict((1024, y_size, x_size))\n",
    "    if crop_info != restricted:\n",
    "        print(\"Cropping outside of bounds, recrop will be different.\" + str(restricted))\n",
    "\n",
    "def validate_dtype(gt_volume):\n",
    "    if gt_volume.dtype != np.uint8 and gt_volume.dtype != np.uint16:\n",
    "        print(\"Annotation has unexpecteed type: \" + str(gt_volume.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate annotation for instance segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_validate_annotations_segmentation = False\n",
    "\n",
    "from clb.image_processing import find_corresponding_labels\n",
    "\n",
    "check_subset_internal_names = ['512']\n",
    "\n",
    "def set_filter(cropping_set):\n",
    "    if check_subset_internal_names is None:\n",
    "        return True\n",
    "    else:\n",
    "        return any([text in cropping_set.internal_name for text in check_subset_internal_names])\n",
    "    \n",
    "\n",
    "def all_datasets_from_training(annotation_root, subfolder):\n",
    "    res = []\n",
    "    for dataset in ['train', 'val', 'test']:\n",
    "        res +=  get_standard_datasets(annotation_root, os.path.join(subfolder, dataset), False)\n",
    "    return res\n",
    "\n",
    "crops_with_big_problems = []\n",
    "\n",
    "if run_validate_annotations_segmentation:\n",
    "    datasets_private = []\n",
    "    groups_private = filter(set_filter, all_segmentation_sets)\n",
    "    \n",
    "    for cropping_set in groups_private:\n",
    "        datasets_private += cropping_set.get_datasets()\n",
    "    \n",
    "    datasets_clb = []\n",
    "    groups = ['T3','T5','T6','T8','NC']\n",
    "    for group in groups:\n",
    "        datasets_clb += all_datasets_from_training(os.path.join(code_root, \"data\\\\training\"), group)\n",
    "    \n",
    "    datasets_current = datasets_private\n",
    "    \n",
    "    for i, d in enumerate(datasets_current):\n",
    "        # validate that it can be read using imageio\n",
    "        print (\"Validating: {0}\\n\\t{1}\".format(i,d.gt))\n",
    "        \n",
    "        if \"_E\" in d.gt:\n",
    "            print (\"Evaluation crops are temporary skipped.\")\n",
    "            continue\n",
    "        \n",
    "        crop_volume = imageio.volread(d.gt)\n",
    "        \n",
    "        # validate that size is 200x200\n",
    "        if crop_volume.shape[1:] not in [(200, 200),(128,128)]:\n",
    "            print(\"Incorrect shape:\", d.gt, crop_volume.shape[1:])\n",
    "            \n",
    "        validate_crop_info_in_bounds(d, 1024, 1024)\n",
    "        validate_dtype(crop_volume)\n",
    "            \n",
    "        # validate that every object is connected\n",
    "        crop_volume_no_blobs = crop_volume.copy() \n",
    "        crop_volume_no_blobs[crop_volume_no_blobs == 1] = 0 # remove blobs\n",
    "        relabel_volume = skimage.measure.label(crop_volume_no_blobs)\n",
    "        for k in range(len(relabel_volume)):\n",
    "            raw = crop_volume_no_blobs[k]\n",
    "            relabeled = relabel_volume[k]\n",
    "            \n",
    "            mapping_12 = find_corresponding_labels(raw, relabeled, return_overlap=True)\n",
    "            overlaps_12 = [(a,b,overlap) for a, (b, overlap) in mapping_12.items() if overlap < 1.0]\n",
    "            \n",
    "            mapping_21 = find_corresponding_labels(relabeled, raw, return_overlap=True)\n",
    "            overlaps_21 = [(a,b,overlap) for a, (b, overlap) in mapping_21.items() if overlap < 1.0]\n",
    "            \n",
    "            if (overlaps_12):\n",
    "                print(\"\\tIncorrect objects in raw-relabel:\", d.gt, \"s={0}\".format(k), overlaps_12)\n",
    "                crops_with_big_problems.append((i, k, overlaps_12[0][0]))\n",
    "            if (overlaps_21):\n",
    "                print(\"\\tIncorrect objects in relabel-raw:\", d.gt, \"s={0}\".format(k), overlaps_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_suspect(file, frame, label):\n",
    "    inspect_gt = datasets_current[file].gt\n",
    "    print (\"File: \", inspect_gt)\n",
    "    print (\"Slice number (first=0): \", frame)\n",
    "    print (\"Label value: \", label)\n",
    "    print ()\n",
    "    crop_volume = imageio.volread(inspect_gt)[frame]\n",
    "    crop_volume[crop_volume!=label] = 0\n",
    "    return crop_volume\n",
    "\n",
    "def get_all_suspects(suspect_list):\n",
    "    return [get_suspect(file=file, frame=frame, label=label) for (file, frame, label) in suspect_list]\n",
    "\n",
    "#show_all(1, 32, get_suspect(file=2, frame=0, label=38), get_suspect(file=6, frame=0, label=21), scale=20)\n",
    "#show_all((len(crops_with_big_problems)+2)//3, 3, scale=5, *get_all_suspects(crops_with_big_problems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate annotation for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clb.classify.utils\n",
    "importlib.reload(clb.classify.utils)\n",
    "\n",
    "run_validate_annotations_classification_pan_ki = False\n",
    "run_validate_annotations_classification_pdl_cd = False\n",
    "\n",
    "def validate_annotation_class(dataset_names, class_name):\n",
    "    print (\"\\nNow validating class:\", class_name, \"\\n\")\n",
    "    datasets_private = []\n",
    "    for group in dataset_names:\n",
    "        datasets_private += get_class_datasets(annotation_root, group, class_name)\n",
    "\n",
    "    \n",
    "    datasets_current = datasets_private\n",
    "    for i, d in enumerate(datasets_current):\n",
    "        gt_path = os.path.relpath(d.gt, annotation_root)\n",
    "        # validate that it can be read using imageio\n",
    "        print (\"Validating: {0}\\n\\t{1}\".format(i,gt_path))\n",
    "        crop_volume = imageio.volread(d.gt)\n",
    "        \n",
    "        # validate that size is 400x400\n",
    "        if np.any(np.array(crop_volume.shape[1:]) < 300):\n",
    "            print(\"Incorrect shape:\", gt_path, crop_volume.shape[1:])\n",
    "            \n",
    "        # validate values 1-2\n",
    "        if np.any(crop_volume > 2):\n",
    "            print(\"Invalid values:\", gt_path, np.unique(crop_volume))\n",
    "            \n",
    "        validate_crop_info_in_bounds(d, 1024, 1024)\n",
    "        validate_dtype(crop_volume)\n",
    "            \n",
    "        # validate only one slice annotated\n",
    "        annotated_slices = [i for i, s in enumerate(crop_volume) if np.any(s)]\n",
    "        if len(annotated_slices) > 1:\n",
    "            print(\"More than one slice annotated:\", gt_path, annotated_slices)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_names = ['T3_S1_counters','T5_S1_counters','T6_S1_counters', \n",
    "                 'T3_S1_extended','T5_S1_extended','T6_S1_extended','T8_S2_extended']\n",
    "\n",
    "if run_validate_annotations_classification_pan_ki:\n",
    "    validate_annotation_class(dataset_names, \"epith\")\n",
    "    validate_annotation_class(dataset_names, \"Ki67\")\n",
    "    \n",
    "dataset_names_pdl_cd = ['TestNewClasses_S1_extended', 'TestNewClasses_S2_extended', 'TestNewClasses_S3_extended',\n",
    "                       'TestNewClasses_S1_test', 'TestNewClasses_S2_test']\n",
    "\n",
    "if run_validate_annotations_classification_pdl_cd:\n",
    "    validate_annotation_class(dataset_names_pdl_cd, \"pdl1\")\n",
    "    validate_annotation_class(dataset_names_pdl_cd, \"cd8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import clb.classify.utils\n",
    "run_compare_ann = False\n",
    "dataset_names_pairs = [('T3_S1_extended', '20180608_T3_S1_subsamples_classes'),\n",
    "                       ('T5_S1_extended','20180608_T5_S1_subsamples_classes'),\n",
    "                       ('T6_S1_extended','20180608_T6_S1_subsamples_classes'),\n",
    "                       ('T8_S2_extended', '20180316_T8_S2_subsamples_classes'),\n",
    "                       \n",
    "                       ('T3_S1_counters', '20180608_T3_S1_subsamples_counting', 'annotations'),\n",
    "                       ('T5_S1_counters', '20180608_T5_S1_subsamples_counting', 'annotations'),\n",
    "                       ('T6_S1_counters', '20180608_T6_S1_subsamples_counting', 'annotations')]\n",
    "\n",
    "dataset_names_pairs_new = [('TestNewClasses_S1_extended', '20181107_TestNewClass_S1_classes', 'annotations'),\n",
    "                           ('TestNewClasses_S2_extended', '20181107_TestNewClass_S2_classes', 'annotations'),\n",
    "                           ('TestNewClasses_S3_extended', '20181107_TestNewClass_S3_classes', 'annotations'),\n",
    "                           ('TestNewClasses_S1_test', '20181107_TestNewClass_S1_classes_test', 'annotations'),\n",
    "                           ('TestNewClasses_S2_test', '20181107_TestNewClass_S2_classes_test', 'annotations')]\n",
    "\n",
    "def compare(path_a,path_b):\n",
    "    name_a = os.path.basename(path_a)\n",
    "    name_b = os.path.basename(path_b)\n",
    "    \n",
    "    print(\"Comparing:\", name_a)\n",
    "    if name_a != name_b:\n",
    "        print (\"\\tDifferent names!\\n\\t\\t\", name_a, \"\\n\\t\\t\", name_b)\n",
    "        \n",
    "    a = imageio.volread(path_a)\n",
    "    b = imageio.volread(path_b)\n",
    "\n",
    "    if a.shape != b.shape or np.any(a!=b):\n",
    "        print (\"\\tDifferent values!\"\n",
    " \n",
    "if run_compare_ann:\n",
    "    for data_tuple in dataset_names_pairs + dataset_names_pairs_new:\n",
    "        raw_dir_name = \"Annotations\"\n",
    "        if len(data_tuple) > 2:\n",
    "            processed_name, raw_name, raw_dir_name = data_tuple\n",
    "        else:\n",
    "            processed_name, raw_name = data_tuple\n",
    "            \n",
    "        processed_path = os.path.join(annotation_root, processed_name, \"annotations\")\n",
    "        raw_path = os.path.join(raw_annotation_root, raw_name, raw_dir_name)\n",
    "        \n",
    "        process_files = clb.classify.utils.find_all_tiffs(processed_path, \"shapes\")\n",
    "        raw_files = clb.classify.utils.find_all_tiffs(raw_path, \"shapes\")\n",
    "        \n",
    "        print (\"Process files:\", len(process_files), \" Raw files:\", len(raw_files))\n",
    "        for processed_tif, raw_tif in zip(process_files, raw_files):\n",
    "            compare(processed_tif, raw_tif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare summary of all annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_prepare_summary = False\n",
    "\n",
    "import clb.evaluate.evaluator_segment\n",
    "importlib.reload(clb.evaluate.evaluator_segment)\n",
    "importlib.reload(clb.cropping)\n",
    "\n",
    "def prepare_summary(name, file_paths):\n",
    "    all_data = [(os.path.basename(f), clb.cropping.CropInfo.load(f)) for f in file_paths]\n",
    "    res = \"DATASET: \" + name + \", number of annotated crops = \" + str(sum([len(cs) for _, cs in all_data]))\n",
    "    res += \", number of volume crops = \" + str(len(all_data))\n",
    "    for n, cs in all_data:\n",
    "        c_first = cs[0]\n",
    "        zs = [c.z for c in cs]\n",
    "        z_step = zs[1] - zs[0]\n",
    "        res += \"\\n\\tCrop: {0}\\n\\t\\tx={1}, y={2}, z={3}, z_step={4}\".format(n, c_first.x, c_first.y, str(zs), z_step)\n",
    "    return res\n",
    "\n",
    "def validate_volume_set(volume_set):\n",
    "    exist_input = os.path.exists(volume_set.input)\n",
    "    exist_gt = os.path.exists(volume_set.gt)\n",
    "    #print (volume_set.input + \"\\n\" + volume_set.gt + \"\\n\")\n",
    "    if not exist_input or not exist_gt:\n",
    "        print (volume_set.input + \" is missing gt? \" + (not exist_gt))\n",
    "\n",
    "def make_summary_file(root_dir, datasets_names, output_name, \n",
    "                      input_name=\"Input\", labels_name=\"Annotations\", get_datasets_fun=None):\n",
    "    output = os.path.join(root_dir, output_name)\n",
    "    with open(output, \"w\") as f:\n",
    "        for dataset in tqdm(datasets_names):\n",
    "            print ('Summary for', dataset + '...')\n",
    "            annotation_base_dir = os.path.join(root_dir, dataset)\n",
    "            input_dir = os.path.join(annotation_base_dir, input_name)\n",
    "            labels_dir = os.path.join(annotation_base_dir, labels_name)\n",
    "            datasets = get_datasets_fun(input_dir, labels_dir)\n",
    "            for s in datasets:\n",
    "                validate_volume_set(s)\n",
    "            print(prepare_summary(dataset, [ d.input[:-4] + '.yaml' for d in datasets ]), file=f)\n",
    "            \n",
    "if run_prepare_summary:\n",
    "    def get_datasets(i,l):\n",
    "        return clb.evaluate.evaluator_segment.get_all_datasets(i,l,i,i)\n",
    "    make_summary_file(annotation_root, ['T3_S1','T5_S1','T6_S1','T8_S1','T8_S2','T8_S3',\n",
    "                                       'TestNewClasses_S1', 'TestNewClasses_S2'], \n",
    "                      \"summary_instances.txt\", get_datasets_fun=get_datasets)\n",
    "\n",
    "if run_prepare_summary:\n",
    "    def get_datasets(i,l):\n",
    "        return clb.classify.utils.get_all_datasets(i,l,i,i,'epith')\n",
    "    make_summary_file(annotation_root, ['T3_S1_extended','T5_S1_extended','T6_S1_extended','T8_S2_extended',\n",
    "                                        'T3_S1_counters', 'T5_S1_counters', 'T6_S1_counters'], \n",
    "                      \"summary_classes_panck_ki67.txt\", input_name=\"input\", labels_name=\"annotations\",\n",
    "                      get_datasets_fun=get_datasets)\n",
    "    \n",
    "if run_prepare_summary:\n",
    "    def get_datasets(i,l):\n",
    "        return clb.classify.utils.get_all_datasets(i,l,i,i,'cd')\n",
    "    make_summary_file(annotation_root, ['TestNewClasses_S1_extended', 'TestNewClasses_S2_extended', 'TestNewClasses_S3_extended',\n",
    "                                       'TestNewClasses_S1_test', 'TestNewClasses_S2_test'], \n",
    "                      \"summary_classes_pdl1_cd8.txt\", input_name=\"input\", labels_name=\"annotations\",\n",
    "                      get_datasets_fun=get_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "427.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
